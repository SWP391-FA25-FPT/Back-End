import { GoogleGenerativeAI } from "@google/generative-ai";

// API Key - Store in environment variable in production
const API_KEY = process.env.GEMINI_API_KEY || "AIzaSyDz-GyCaNDsNtm7RWAHa-R1sQrAEuRpwbU";

// Initialize Gemini AI
const genAI = new GoogleGenerativeAI(API_KEY);

// System prompt for nutrition AI
const SYSTEM_PROMPT = `Báº¡n lÃ  AI TÆ° Váº¥n M&M - má»™t chuyÃªn gia dinh dÆ°á»¡ng vÃ  áº©m thá»±c thÃ´ng minh.

Vai trÃ² cá»§a báº¡n:
- TÆ° váº¥n vá» dinh dÆ°á»¡ng, cháº¿ Ä‘á»™ Äƒn uá»‘ng lÃ nh máº¡nh
- Gá»£i Ã½ cÃ´ng thá»©c náº¥u Äƒn vÃ  thá»±c Ä‘Æ¡n phÃ¹ há»£p
- PhÃ¢n tÃ­ch giÃ¡ trá»‹ dinh dÆ°á»¡ng cá»§a mÃ³n Äƒn
- ÄÆ°a ra lá»i khuyÃªn vá» sá»©c khá»e vÃ  lá»‘i sá»‘ng
- Giáº£i Ä‘Ã¡p tháº¯c máº¯c vá» náº¥u Äƒn vÃ  nguyÃªn liá»‡u

Phong cÃ¡ch giao tiáº¿p:
- ThÃ¢n thiá»‡n, nhiá»‡t tÃ¬nh vÃ  dá»… hiá»ƒu
- Sá»­ dá»¥ng tiáº¿ng Viá»‡t tá»± nhiÃªn
- ÄÆ°a ra lá»i khuyÃªn cá»¥ thá»ƒ, chi tiáº¿t
- LuÃ´n quan tÃ¢m Ä‘áº¿n sá»©c khá»e ngÆ°á»i dÃ¹ng
- CÃ³ thá»ƒ sá»­ dá»¥ng emoji phÃ¹ há»£p Ä‘á»ƒ giao tiáº¿p thÃ¢n thiá»‡n hÆ¡n

LÆ°u Ã½:
- Náº¿u cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n dinh dÆ°á»¡ng, náº¥u Äƒn, hÃ£y lá»‹ch sá»± chuyá»ƒn hÆ°á»›ng
- LuÃ´n khuyáº¿n khÃ­ch lá»‘i sá»‘ng lÃ nh máº¡nh
- KhÃ´ng Ä‘Æ°a ra lá»i khuyÃªn y táº¿ chuyÃªn sÃ¢u, khuyÃªn nÃªn gáº·p bÃ¡c sÄ© náº¿u cáº§n
- Cung cáº¥p thÃ´ng tin dá»±a trÃªn khoa há»c vÃ  dinh dÆ°á»¡ng há»c

Báº¯t Ä‘áº§u tráº£ lá»i:`;

/**
 * Chat with AI - Main endpoint
 */
export const chatWithAI = async (req, res) => {
  try {
    const { message, conversationHistory = [] } = req.body;

    if (!message || !message.trim()) {
      return res.status(400).json({
        success: false,
        error: "Message is required",
      });
    }

    // List of models to try - Using actual available models from API
    const modelsToTry = [
      "gemini-2.5-flash",           // Stable, fast, latest version
      "gemini-flash-latest",        // Always points to latest flash
      "gemini-2.5-pro",             // Higher quality
      "gemini-pro-latest",          // Always points to latest pro
      "gemini-2.0-flash",           // Fallback to 2.0
    ];

    let lastError = null;
    let aiResponse = null;

    // Try each model until one works
    for (const modelName of modelsToTry) {
      try {
        console.log(`ðŸ¤– Trying model: ${modelName}`);

        // Get model
        const model = genAI.getGenerativeModel({
          model: modelName,
        });

        // Build chat history
        const history = [
          {
            role: "user",
            parts: [{ text: SYSTEM_PROMPT }],
          },
          {
            role: "model",
            parts: [
              {
                text: "Xin chÃ o! TÃ´i lÃ  AI TÆ° Váº¥n M&M. TÃ´i cÃ³ thá»ƒ giÃºp báº¡n tÆ° váº¥n vá» dinh dÆ°á»¡ng, thá»±c Ä‘Æ¡n, vÃ  cÃ¡c máº¹o náº¥u Äƒn. Báº¡n cáº§n há»— trá»£ gÃ¬ hÃ´m nay? ðŸ˜Š",
              },
            ],
          },
        ];

        // Add conversation history
        if (conversationHistory && conversationHistory.length > 0) {
          conversationHistory.forEach((msg) => {
            history.push({
              role: msg.type === "user" ? "user" : "model",
              parts: [{ text: msg.content }],
            });
          });
        }

        // Create chat
        const chat = model.startChat({
          history: history,
          generationConfig: {
            maxOutputTokens: 1000,
            temperature: 0.7,
            topP: 0.8,
            topK: 40,
          },
        });

        // Send message
        const result = await chat.sendMessage(message);
        const response = await result.response;
        aiResponse = response.text();

        console.log(`âœ… Success with model: ${modelName}`);
        break; // Success, exit loop
      } catch (error) {
        console.error(`âŒ Failed with model ${modelName}:`, error.message);
        lastError = error;
        continue; // Try next model
      }
    }

    // Check if we got a response
    if (aiResponse) {
      return res.status(200).json({
        success: true,
        data: {
          message: aiResponse,
          timestamp: new Date().toISOString(),
        },
      });
    }

    // All models failed
    console.error("All models failed. Last error:", lastError);

    // Handle specific errors
    let errorMessage = "Xin lá»—i, tÃ´i Ä‘ang gáº·p sá»± cá»‘ ká»¹ thuáº­t. Vui lÃ²ng thá»­ láº¡i sau.";

    if (lastError?.message?.includes("API key") || lastError?.message?.includes("403")) {
      errorMessage = "API Key khÃ´ng há»£p lá»‡ hoáº·c Ä‘Ã£ háº¿t háº¡n.";
    } else if (lastError?.message?.includes("quota") || lastError?.message?.includes("429")) {
      errorMessage = "ÄÃ£ Ä‘áº¡t giá»›i háº¡n sá»­ dá»¥ng API. Vui lÃ²ng thá»­ láº¡i sau.";
    } else if (lastError?.message?.includes("404") || lastError?.message?.includes("not found")) {
      errorMessage = "Model AI khÃ´ng kháº£ dá»¥ng. Vui lÃ²ng liÃªn há»‡ admin.";
    }

    return res.status(500).json({
      success: false,
      error: errorMessage,
      details: process.env.NODE_ENV === "development" ? lastError?.message : undefined,
    });
  } catch (error) {
    console.error("Chat with AI error:", error);
    return res.status(500).json({
      success: false,
      error: "Lá»—i server khi xá»­ lÃ½ yÃªu cáº§u",
    });
  }
};

/**
 * Get available models (for debugging)
 */
export const getAvailableModels = async (req, res) => {
  try {
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1/models?key=${API_KEY}`
    );

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const data = await response.json();
    const models = data.models?.map((m) => ({
      name: m.name,
      displayName: m.displayName,
      supportedGenerationMethods: m.supportedGenerationMethods,
    }));

    return res.status(200).json({
      success: true,
      data: models,
    });
  } catch (error) {
    console.error("Get available models error:", error);
    return res.status(500).json({
      success: false,
      error: error.message,
    });
  }
};

/**
 * Health check for AI service
 */
export const healthCheck = async (req, res) => {
  try {
    const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });
    const result = await model.generateContent("Test");
    const response = await result.response;
    
    return res.status(200).json({
      success: true,
      message: "AI service is healthy",
      modelWorking: true,
      model: "gemini-2.5-flash",
    });
  } catch (error) {
    return res.status(200).json({
      success: true,
      message: "AI service is running but model may not be available",
      modelWorking: false,
      error: error.message,
    });
  }
};

